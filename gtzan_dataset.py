# -*- coding: utf-8 -*-
"""Gtzan_DataSet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KoOWd-GRFFATJZd6DNqtu9KQlQQJY_dV
"""

from google.colab import drive
drive.mount('/content/drive')

import kagglehub

# Download latest version
path = kagglehub.dataset_download("andradaolteanu/gtzan-dataset-music-genre-classification")

print("Path to dataset files:", path)

# Commented out IPython magic to ensure Python compatibility.
# Genel libaryler
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import sklearn

# Librosa libaryleri
import librosa
import librosa.display
import IPython.display as ipd
import warnings
warnings.filterwarnings('ignore')

import os
general_path = '/content/drive/MyDrive/Colab Notebooks/Data'
print(list(os.listdir(f'{general_path}/genres_original/')))

import os
Root = '/content/drive/My Drive/Colab Notebooks'
os.chdir(Root)


print("Current working directory:", os.getcwd())
print("Files in the directory:", os.listdir())

y, sr = librosa.load(f'{general_path}/genres_original/metal/metal.00034.wav')

print('y:', y, '\n')
print('y shape:', np.shape(y), '\n')
print('Sample Rate (KHz):', sr, '\n')

#Şarkı süresinin doğruluğu
print('Check Len of Audio:', 661794/22050)

audio_file, _ = librosa.effects.trim(y)

#Numpy ndarray çıktısı
print('Audio File:', audio_file, '\n')
print('Audio File shape:', np.shape(audio_file))

plt.figure(figsize = (16, 6))
librosa.display.waveshow(audio_file, sr = sr, color = "#800000");
plt.title("Trooper Iron Maiden", fontsize = 23);

# FFT Penceresinin boyutları
n_fft = 2048 # FFT pencere boyutu
hop_length = 512 #STFT sütünları için kullanılacak boşluk miktarı (Tamamen bi estimate üzerinden)

#(STFT)
D = np.abs(librosa.stft(y=audio_file, n_fft = n_fft, hop_length = hop_length))

print('Shape of D object:', np.shape(D))

plt.figure(figsize = (16, 6))
plt.plot(D);

DB = librosa.amplitude_to_db(D, ref = np.max)

# Creating the Spectogram
plt.figure(figsize = (16, 6))
librosa.display.specshow(DB, sr = sr, hop_length = hop_length, x_axis = 'time', y_axis = 'log',
                        cmap ='icefire')

plt.colorbar();
plt.title("Trooper Iron Maiden", fontsize = 23);

y, sr = librosa.load(f'{general_path}/genres_original/metal/metal.00034.wav')
y, _ = librosa.effects.trim(y)


S = librosa.feature.melspectrogram(y=y, sr=sr)
S_DB = librosa.amplitude_to_db(S, ref=np.max)
plt.figure(figsize = (16, 6))
librosa.display.specshow(S_DB, sr=sr, hop_length=hop_length, x_axis = 'time', y_axis = 'log',
                        cmap = 'icefire');
plt.colorbar();
plt.title("Mel Spectogram", fontsize = 23);

y, sr = librosa.load(f'{general_path}/genres_original/rock/rock.00040.wav')
y, _ = librosa.effects.trim(y)


S = librosa.feature.melspectrogram(y=y, sr=sr)
S_DB = librosa.amplitude_to_db(S, ref=np.max)
plt.figure(figsize = (16, 6))
librosa.display.specshow(S_DB, sr=sr, hop_length=hop_length, x_axis = 'time', y_axis = 'log',
                        cmap = 'icefire');
plt.colorbar();
plt.title("Classical Mel Spectrogram", fontsize = 23);

y, sr = librosa.load(f'{general_path}/genres_original/metal/metal.00034.wav')
audio_file, _ = librosa.effects.trim(y)

# Total zero_crossings in our 1 song
zero_crossings = librosa.zero_crossings(audio_file, pad=False)
print(sum(zero_crossings))

y_harm, y_perc = librosa.effects.hpss(audio_file)

plt.figure(figsize = (16, 6))
plt.title("IronMaiden Trooper", fontsize = 23);
plt.plot(y_harm, color = '#A300F9');
plt.plot(y_perc, color = '#FFB100');

y, sr = librosa.load(f'{general_path}/genres_original/metal/metal.00034.wav')
# Adjust the start_bpm parameter to a more appropriate range for metal
tempo, _ = librosa.beat.beat_track(y=y, sr = sr)
tempo

# Increase or decrease hop_length to change how granular you want your data to be
hop_length = 5000
y, sr = librosa.load(f'{general_path}/genres_original/metal/metal.00034.wav')
audio_file, _ = librosa.effects.trim(y)
# Chromogram
chromagram = librosa.feature.chroma_stft(y=audio_file, sr=sr, hop_length=hop_length)
print('Chromogram shape:', chromagram.shape)
print('Chromogram mean:', chromagram.mean())
print('Chromogram var:', chromagram.var())

plt.figure(figsize=(16, 6))
plt.title("IronMaiden Trooper", fontsize = 23);
librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm');

y, sr = librosa.load(f'{Root}/Seytan30S.wav')
mfccs = librosa.feature.mfcc(y=audio_file, sr=sr)
print('mfccs shape:', mfccs.shape)

#Displaying  the MFCCs:
plt.figure(figsize = (16, 6))
librosa.display.specshow(mfccs, sr=sr, x_axis='time', cmap = 'cool');

import librosa
import librosa.display
import numpy as np

def get_mfcc_mean_var(audio_file, sr, num_mfccs=20, frame_length=2048, hop_length=512):
    """
    Calculates the mean and variance of MFCCs for a given audio file.

    Args:
        audio_file (np.ndarray): The audio signal.
        sr (int): The sample rate of the audio signal.
        num_mfccs (int): The number of MFCCs to calculate.
        frame_length (int): The length of each frame in samples.
        hop_length (int): The number of samples to hop between frames.

    Returns:
        tuple: A tuple containing two NumPy arrays:
            - mfcc_means: The mean of each MFCC across all frames.
            - mfcc_vars: The variance of each MFCC across all frames.
    """

    # Calculate MFCCs for the entire audio file
    mfccs = librosa.feature.mfcc(y=audio_file, sr=sr, n_mfcc=num_mfccs,
                                     n_fft=frame_length, hop_length=hop_length)

    # Calculate mean and variance across frames
    mfcc_means = np.mean(mfccs, axis=1)
    mfcc_vars = np.var(mfccs, axis=1)

    return mfcc_means, mfcc_vars

# Load your audio file
y, sr = librosa.load(f'{Root}/Seytan30S.wav')
audio_file, _ = librosa.effects.trim(y)

# Get 20 MFCC mean and variance values
mfcc_means, mfcc_vars = get_mfcc_mean_var(audio_file, sr, num_mfccs=20)

# Print the results
print("MFCC Means:", mfcc_means)
print("MFCC Variances:", mfcc_vars)

y, sr = librosa.load('/content/drive/MyDrive/Colab Notebooks/Data/genres_original/blues/blues.00001.wav')

# Calculate RMS values for frames
rms = librosa.feature.rms(y=y)[0]

# Calculate rms_mean and rms_var
rms_mean = np.mean(rms)
rms_var = np.var(rms)

 # Print the results
print("rms_mean:", rms_mean)
print("rms_var:", rms_var)

data = pd.read_csv(f'{general_path}/features_30_sec.csv')
data.head()

# Computing the Correlation Matrix
spike_cols = [col for col in data.columns if 'mean' in col]
corr = data[spike_cols].corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(16, 11));

# Generate a custom diverging colormap
cmap = sns.diverging_palette(0, 25, as_cmap=True, s = 90, l = 45, n = 5)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

plt.title('Correlation Heatmap (for the MEAN variables)', fontsize = 25)
plt.xticks(fontsize = 10)
plt.yticks(fontsize = 10);
plt.savefig("Corr Heatmap.jpg")

x = data[["label", "tempo"]]

f, ax = plt.subplots(figsize=(16, 9));
sns.boxplot(x = "label", y = "tempo", data = x, palette = 'husl');

plt.title('BPM Boxplot for Genres', fontsize = 25)
plt.xticks(fontsize = 14)
plt.yticks(fontsize = 10);
plt.xlabel("Genre", fontsize = 15)
plt.ylabel("BPM", fontsize = 15)
plt.savefig("BPM Boxplot.jpg")

from sklearn import preprocessing

data = data.iloc[0:, 1:]
y = data['label']
X = data.loc[:, data.columns != 'label']

#### NORMALIZE X ####
cols = X.columns
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)
X = pd.DataFrame(np_scaled, columns = cols)


#### PCA 2 COMPONENTS ####
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
principalComponents = pca.fit_transform(X)
principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])

# concatenate with target label
finalDf = pd.concat([principalDf, y], axis = 1)

pca.explained_variance_ratio_

# 44.93 variance explained

plt.figure(figsize = (16, 9))
sns.scatterplot(x = "principal component 1", y = "principal component 2", data = finalDf, hue = "label", alpha = 0.7,
               s = 100);

plt.title('PCA on Genres', fontsize = 25)
plt.xticks(fontsize = 14)
plt.yticks(fontsize = 10);
plt.xlabel("Principal Component 1", fontsize = 15)
plt.ylabel("Principal Component 2", fontsize = 15)
plt.savefig("PCA Scattert.jpg")

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier, XGBRFClassifier
from xgboost import plot_tree, plot_importance

from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE

data = pd.read_csv(f'{general_path}/features_3_sec.csv')
data = data.iloc[0:, 1:]
data.head()

y = data['label'] # genre variable.
X = data.loc[:, data.columns != 'label'] #select all columns but not the labels

#### NORMALIZE X ####

# Normalize so everything is on the same scale.

cols = X.columns
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)

# new data frame with the new scaled data.
X = pd.DataFrame(np_scaled, columns = cols)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Import necessary libraries
from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder object
le = LabelEncoder()

# Fit the encoder to your training labels and transform them
y_train = le.fit_transform(y_train)

# Transform your test labels using the same encoder
y_test = le.transform(y_test)

# Now, you can proceed with model training
def model_assess(model, title="Default"):
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    # print(confusion_matrix(y_test, preds))
    print('Accuracy', title, ':', round(accuracy_score(y_test, preds), 5), '\n')

# Naive Bayes
nb = GaussianNB()
model_assess(nb, "Naive Bayes")

# Stochastic Gradient Descent
sgd = SGDClassifier(max_iter=5000, random_state=0)
model_assess(sgd, "Stochastic Gradient Descent")

# KNN
knn = KNeighborsClassifier(n_neighbors=19)
model_assess(knn, "KNN")

# Decission trees
tree = DecisionTreeClassifier()
model_assess(tree, "Decission trees")

# Random Forest
rforest = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)
model_assess(rforest, "Random Forest")

# Support Vector Machine
svm = SVC(decision_function_shape="ovo")#SvMnin çeşitlerine bakalım. Lineer Kübik RBF kernel
model_assess(svm, "Support Vector Machine")

# Logistic Regression
lg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')
model_assess(lg, "Logistic Regression")

# Neural Nets
nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1)
model_assess(nn, "Neural Nets")

# Cross Gradient Booster
xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)
model_assess(xgb, "Cross Gradient Booster")

# Cross Gradient Booster (Random Forest)
xgbrf = XGBRFClassifier(objective= 'multi:softmax')
model_assess(xgbrf, "Cross Gradient Booster (Random Forest)")

# KNN
knn = KNeighborsClassifier(n_neighbors=19)
model_assess(knn, "KNN")

# Random Forest
rforest = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)
model_assess(rforest, "Random Forest")

# Support Vector Machine
svm = SVC(decision_function_shape="ovo")
model_assess(svm, "Support Vector Machine")

# Cross Gradient Booster
xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)
model_assess(xgb, "Cross Gradient Booster")

# Cross Gradient Booster (Random Forest)
xgbrf = XGBRFClassifier(objective= 'multi:softmax')
model_assess(xgbrf, "Cross Gradient Booster (Random Forest)")

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Define the parameter grid to search
param_grid = {
    'n_neighbors': list(range(1, 31, 2)) # Try odd numbers from 1 to 30
}

# Create a KNN classifier object
knn = KNeighborsClassifier()

# Create GridSearchCV object
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid,
                           scoring='accuracy', cv=5, n_jobs=-1)

# Fit the grid search to the training data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and the corresponding accuracy
print("Best n_neighbors:", grid_search.best_params_['n_neighbors'])
print("Best accuracy:", grid_search.best_score_)

# Now you can use the best model for predictions
best_knn = grid_search.best_estimator_
preds = best_knn.predict(X_test)
print('Accuracy on test data:', round(accuracy_score(y_test, preds), 5))

from xgboost import XGBClassifier
xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05, reg_lambda=1.0, reg_alpha=0.1) # reg_lambda is L2 regularization
model_assess(xgb, "Cross Gradient Booster")

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Create and train the KNN model (using the best hyperparameters if tuned)
knn = KNeighborsClassifier(n_neighbors=1)  # Or use best_knn from GridSearchCV
knn.fit(X_train, y_train)

# Make predictions on the test data
preds = knn.predict(X_test)

# Calculate and print the accuracy
print('Accuracy KNN:', round(accuracy_score(y_test, preds), 5), '\n')

# Generate the confusion matrix
confusion_matr = confusion_matrix(y_test, preds)

# Display the confusion matrix using seaborn heatmap
plt.figure(figsize=(16, 9))
sns.heatmap(confusion_matr, cmap="Blues", annot=True,
            xticklabels=["blues", "classical", "country", "disco", "hiphop", "jazz", "metal", "pop", "reggae", "rock"],
            yticklabels=["blues", "classical", "country", "disco", "hiphop", "jazz", "metal", "pop", "reggae", "rock"])
plt.title('Confusion Matrix for KNN', fontsize=25)  # Add a title
plt.xlabel('Predicted Genre', fontsize=15)  # Add x-axis label
plt.ylabel('True Genre', fontsize=15)  # Add y-axis label
plt.savefig("conf_matrix_knn.jpg")  # Save the figure
plt.show()  # Show the plot

"""Feature Selection ve Modellerin parametreleriyle oynayarak daha iyi performans elde edilmesi."""

xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)
xgb.fit(X_train, y_train)


preds = xgb.predict(X_test)

print('Accuracy', ':', round(accuracy_score(y_test, preds), 5), '\n')

# Confusion Matrix
confusion_matr = confusion_matrix(y_test, preds) #normalize = 'true'
plt.figure(figsize = (16, 9))
sns.heatmap(confusion_matr, cmap="Blues", annot=True,
            xticklabels = ["blues", "classical", "country", "disco", "hiphop", "jazz", "metal", "pop", "reggae", "rock"],
           yticklabels=["blues", "classical", "country", "disco", "hiphop", "jazz", "metal", "pop", "reggae", "rock"]);

import librosa
import numpy as np
import pandas as pd

def predict_genre(audio_path, model, feature_names):
    """
    Predicts the genre of an audio file.

    Args:
        audio_path (str): Path to the audio file.
        model (object): The trained XGBoost model.
        feature_names (list): List of feature names used in the model.

    Returns:
        str: The predicted genre.
    """

    # Load audio file and trim silence
    # Example: Resampling to 22.05 kHz
    y, sr = librosa.load(audio_path, sr=22050)  # Ensure uniform sampling rate
    audio_file, _ = librosa.effects.trim(y)  # Remove silence


    # Feature extraction
    features = []

    # 1. Length
    features.append(len(audio_file))

    # 2. Chroma stft
    chroma_stft = librosa.feature.chroma_stft(y=audio_file, sr=sr)
    features.append(np.mean(chroma_stft))
    features.append(np.var(chroma_stft))

    # 3. RMS
    rms = librosa.feature.rms(y=audio_file)[0]
    features.append(np.mean(rms))
    features.append(np.var(rms))

    # 4. Spectral Centroid
    spectral_centroids = librosa.feature.spectral_centroid(y=audio_file, sr=sr)[0]
    features.append(np.mean(spectral_centroids))
    features.append(np.var(spectral_centroids))

    # 5. Spectral Bandwidth
    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_file, sr=sr)[0]
    features.append(np.mean(spectral_bandwidth))
    features.append(np.var(spectral_bandwidth))

    # 6. Rolloff
    rolloff = librosa.feature.spectral_rolloff(y=audio_file, sr=sr)[0]
    features.append(np.mean(rolloff))
    features.append(np.var(rolloff))

    # 7. Zero Crossing Rate
    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=audio_file)[0]
    features.append(np.mean(zero_crossing_rate))
    features.append(np.var(zero_crossing_rate))

    # 8. Harmony and Perceptr
    harmony, perceptr = librosa.effects.hpss(audio_file)
    features.append(np.mean(harmony))
    features.append(np.var(harmony))
    features.append(np.mean(perceptr))
    features.append(np.var(perceptr))

    # 9. Tempo (normalized single value)
    onset_env = librosa.onset.onset_strength(y=audio_file, sr=sr)
    tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)[0]
    print(f"Extracted tempo: {tempo} BPM")
    features.append(tempo)

    # 10. MFCCs
    mfccs = librosa.feature.mfcc(y=audio_file, sr=sr, n_mfcc=20)
    for i in range(20):
        features.append(np.mean(mfccs[i]))
        features.append(np.var(mfccs[i]))

    # Create a DataFrame for the features
    features_df = pd.DataFrame([features], columns=feature_names)

    # Normalize the features (using the same scaler as before)
    cols = features_df.columns
    features_df = pd.DataFrame(min_max_scaler.transform(features_df), columns=cols)

    # Make prediction
    prediction = model.predict(features_df)[0]

    # Decode prediction to genre label
    genre = le.inverse_transform([prediction])[0]  # Assuming 'le' is your LabelEncoder

    return genre

# Get feature names from the training data
feature_names = X_train.columns.tolist()

# Path to your music file
your_music_path = '/content/drive/MyDrive/Colab Notebooks/Maiden30S.wav'
#your_music_path = '/content/drive/MyDrive/Colab Notebooks/Data/genres_original/rock/rock.00001.wav'

# Predict the genre
predicted_genre = predict_genre(your_music_path, xgb, feature_names)

print(f"Predicted genre: {predicted_genre}")

"""Burada Deneme Amacıyla yeni bi traine geçiyorum ////
/\AS
As
As

1.   Liste öğesi
2.   Liste öğesi


"""

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score
from xgboost import XGBClassifier

# Load and preprocess data
data = pd.read_csv(f'{general_path}/features_3_sec.csv')
data = data.iloc[:, 1:]  # Remove the index column

# Separate features and labels
y = data['label']
X = data.drop('label', axis=1)

# Normalize features
from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler()
X_scaled = min_max_scaler.fit_transform(X)
X = pd.DataFrame(X_scaled, columns=X.columns)

# Encode labels
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# Use StratifiedKFold for cross-validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
xgb = XGBClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=1.0,
    reg_alpha=0.1
)

# Perform cross-validation
cv_scores = cross_val_score(xgb, X_train, y_train, cv=skf, scoring='accuracy')
print(f"Cross-Validation Accuracy: {cv_scores.mean():.5f} ± {cv_scores.std():.5f}")

# Train on full training set and evaluate
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)

# Evaluate performance
test_acc = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {test_acc:.5f}")
print(classification_report(y_test, y_pred, target_names=le.classes_))

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Function to plot feature importance
def plot_feature_importance(model, feature_names, top_n=20):
    """
    Plots the top N important features of the trained model.

    Args:
        model: Trained XGBoost model.
        feature_names: List of feature names.
        top_n: Number of top features to display.
    """
    # Get feature importance scores
    importance = model.feature_importances_

    # Create a DataFrame for easier handling
    feature_importance = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importance
    }).sort_values(by='Importance', ascending=False)

    # Select the top N features
    top_features = feature_importance.head(top_n)

    # Plot the feature importance
    plt.figure(figsize=(10, 6))
    plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')
    plt.gca().invert_yaxis()  # Highest importance on top
    plt.title('Top Feature Importance')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.show()

    return feature_importance

# Call the function to plot feature importance
feature_importance_df = plot_feature_importance(xgb, X.columns.tolist())

# Save feature importance to a CSV file for further analysis (optional)
feature_importance_df.to_csv('feature_importance.csv', index=False)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV

# Define a KNN model
knn = KNeighborsClassifier()

# Hyperparameter tuning with GridSearchCV
param_grid = {
    'n_neighbors': [1],  # Number of neighbors to consider
    'weights': ['uniform', 'distance'],  # Weight function
    'metric': ['euclidean', 'manhattan', 'minkowski']  # Distance metrics
}

# Grid Search for finding the best parameters
grid_search = GridSearchCV(
    estimator=knn,
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,  # 5-fold cross-validation
    verbose=1,
    n_jobs=-1
)

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best KNN model
best_knn = grid_search.best_estimator_
print(f"Best Parameters: {grid_search.best_params_}")

# Test set predictions
y_pred = best_knn.predict(X_test)

# Evaluate the model
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {test_accuracy:.5f}")
print(classification_report(y_test, y_pred, target_names=le.classes_))

import librosa
import numpy as np

def extract_features(audio_path, target_sr=22050):
    """
    Extracts 60 features from an audio file.

    Args:
        audio_path (str): Path to the audio file.
        target_sr (int): Target sampling rate for resampling.

    Returns:
        list: A list of 60 extracted features.
    """
    try:
        # Load the audio file
        y, sr = librosa.load(audio_path, sr=target_sr)  # Resample to target_sr
        y, _ = librosa.effects.trim(y)  # Trim silence
        y = librosa.util.normalize(y)  # Normalize loudness
    except FileNotFoundError:
        print(f"Error: Audio file not found at path: {audio_path}")
        return None

    features = []

    # 1. Length
    features.append(len(y))

    # 2. Chroma stft
    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)
    features.append(np.mean(chroma_stft))
    features.append(np.var(chroma_stft))

    # 3. RMS
    rms = librosa.feature.rms(y=y)[0]
    features.append(np.mean(rms))
    features.append(np.var(rms))

    # 4. Spectral Centroid
    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
    features.append(np.mean(spectral_centroids))
    features.append(np.var(spectral_centroids))

    # 5. Spectral Bandwidth
    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]
    features.append(np.mean(spectral_bandwidth))
    features.append(np.var(spectral_bandwidth))

    # 6. Rolloff
    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
    features.append(np.mean(rolloff))
    features.append(np.var(rolloff))

    # 7. Zero Crossing Rate
    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=y)[0]
    features.append(np.mean(zero_crossing_rate))
    features.append(np.var(zero_crossing_rate))

    # 8. Harmony and Perceptr
    harmony, perceptr = librosa.effects.hpss(y)
    features.append(np.mean(harmony))
    features.append(np.var(harmony))
    features.append(np.mean(perceptr))
    features.append(np.var(perceptr))

    # 9. Tempo
    onset_env = librosa.onset.onset_strength(y=y, sr=sr)
    tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)[0]
    print(f"Extracted tempo: {tempo} BPM")
    features.append(tempo)

    # 10. MFCCs
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)
    for i in range(20):
        features.append(np.mean(mfccs[i]))
        features.append(np.var(mfccs[i]))

    return features



# Extract features for both versions
dataset_features = extract_features('/content/drive/MyDrive/Colab Notebooks/Data/genres_original/metal/metal.00034.wav')
youtube_features = extract_features('/content/drive/MyDrive/Colab Notebooks/IrMa30S.wav')

# Compare features
feature_diff = np.abs(np.array(dataset_features) - np.array(youtube_features))
print("Feature Differences:", feature_diff)

